package metric

import (
	"encoding/json"
	"fmt"
	"io"
	"log/slog"
	"os"
	"path/filepath"
	"regexp"
	"strings"
	"time"
)

type SeriesID struct {
	id       string
	title    string
	maxCount int
	period   time.Duration
}

func (id SeriesID) MarshalJSON() ([]byte, error) {
	return json.Marshal(struct {
		ID       string        `json:"id"`
		Title    string        `json:"title"`
		MaxCount int           `json:"max_count"`
		Period   time.Duration `json:"period"`
	}{
		ID:       id.id,
		Title:    id.title,
		MaxCount: id.maxCount,
		Period:   id.period,
	})
}

func (id *SeriesID) UnmarshalJSON(data []byte) error {
	obj := struct {
		ID       string        `json:"id"`
		Title    string        `json:"title"`
		MaxCount int           `json:"max_count"`
		Period   time.Duration `json:"period"`
	}{}
	if err := json.Unmarshal(data, &obj); err != nil {
		return err
	}
	id.id = obj.ID
	id.title = obj.Title
	id.maxCount = obj.MaxCount
	id.period = obj.Period
	return nil
}

var regexpInvalidSeriesID = regexp.MustCompile(` [\\/:*?">?|\x00-\x1F]`)
var regexpValidSeriesID = regexp.MustCompile("^[A-Z][A-Z0-9_]*[A-Z0-9]+$")

// NewSeriesID creates a new SeriesID from the given id, title, period, and maxCount.
//
// The id is validated to ensure it contains only uppercase letters, numbers, and underscores,
// and starts with a letter. Invalid characters are replaced with underscores.
//
// The title is a human-readable name for the series.
//
// The period is the duration of each data point in the series.
//
// The maxCount is the maximum number of data points to retain in the series.
func NewSeriesID(id string, title string, period time.Duration, maxCount int) (SeriesID, error) {
	// ensure the ID is uppercase and trimmed
	// and validate it
	id = regexpInvalidSeriesID.ReplaceAllString(id, "_")
	id = strings.ToUpper(id)
	ret := SeriesID{
		id:       id,
		title:    title,
		maxCount: maxCount,
		period:   period,
	}
	ret.id = strings.ToUpper(strings.TrimSpace(id))
	if !regexpValidSeriesID.MatchString(ret.id) {
		return ret, fmt.Errorf("invalid series ID %q", id)
	}
	return ret, nil
}

func (id SeriesID) Title() string {
	return id.title
}

func (id SeriesID) ID() string {
	return id.id
}

func (id SeriesID) Period() time.Duration {
	return id.period
}

func (id SeriesID) MaxCount() int {
	return id.maxCount
}

func (id SeriesID) OldestTime() time.Time {
	now := time.Now()
	now = now.Add(id.period / 2).Round(id.period)
	return now.Add(-id.period * time.Duration(id.maxCount))
}

type Storage interface {
	// Store saves the Product generated by the given timeseries.
	// If closing is true, the Product may be incomplete for the current period,
	// as it includes any remaining data when the timeseries is closed.
	Store(id SeriesID, pd Product, closing bool) error

	// Load retrieves up to maxCount of the most recent Products for the given seriesId.
	// If no Products are found, returns (nil, nil).
	Load(id SeriesID, metricName string) ([]Product, error)
}

func NewFileStorage(dir string, bufferSize int) *FileStorage {
	if dir == "" {
		return nil
	}
	if bufferSize <= 0 {
		bufferSize = 100
	}
	return &FileStorage{
		dir:                     dir,
		wChan:                   make(chan *FileRecord, bufferSize),
		closeChan:               make(chan interface{}),
		files:                   make(map[string]*FileHandle),
		shrinkThresholdDuration: time.Minute,
	}
}

var _ Storage = (*FileStorage)(nil)

type FileStorage struct {
	dir       string
	wChan     chan *FileRecord
	closeChan chan interface{}
	files     map[string]*FileHandle

	shrinkThresholdDuration time.Duration
}

type FileRecord struct {
	id      SeriesID
	pd      Product
	closing bool
}

type FileHandle struct {
	file            *os.File
	path            string
	lastAppendCount int64
	lastShrinkTime  time.Time
}

func (ds *FileStorage) Store(id SeriesID, pd Product, closing bool) error {
	ds.wChan <- &FileRecord{id: id, pd: pd, closing: closing}
	return nil
}

func (ds *FileStorage) Open() error {
	slog.Debug("Opening file storage", "dir", ds.dir)
	entry, err := os.ReadDir(ds.dir)
	if err != nil {
		return err
	}
	for _, e := range entry {
		if !e.IsDir() && strings.HasSuffix(e.Name(), ".ts") {
			path := filepath.Join(ds.dir, e.Name())
			// copy .ts files to .ts.bak
			newPath := path + ".bak"
			src, err := os.Open(path)
			if err != nil {
				slog.Error("Failed to open file for backup", "file", path, "error", err)
				continue
			}
			dst, err := os.Create(newPath)
			if err != nil {
				slog.Error("Failed to create backup file", "file", newPath, "error", err)
				src.Close()
				continue
			}
			_, err = io.Copy(dst, src)
			if err != nil {
				slog.Error("Failed to copy file to backup", "src", path, "dst", newPath, "error", err)
			}
			src.Close()
			dst.Close()
			slog.Debug("Backed up file", "src", path, "dst", newPath)
		}
	}
	go ds.runWriteLoop()
	return nil
}

func (ds *FileStorage) Close() error {
	slog.Debug("Closing file storage", "dir", ds.dir)
	close(ds.closeChan)
	for _, h := range ds.files {
		h.file.Close()
	}
	return nil
}

func (ds *FileStorage) runWriteLoop() {
	for {
		select {
		case fr := <-ds.wChan:
			if fr != nil {
				ds.write(fr.id, fr.pd, fr.closing)
			}
		case <-ds.closeChan:
			return
		}
	}
}

// write is called by runLoop goroutine only
// so no need to thread-safeness
func (ds *FileStorage) write(id SeriesID, pd Product, closing bool) error {
	// JSON marshalling
	line, err := json.Marshal(pd)
	if err != nil {
		return err
	}

	h, ok := ds.files[id.ID()]
	if !ok {
		path := filepath.Join(ds.dir, fmt.Sprintf("%s.ts", id.ID()))

		// open file (append)
		f, err := os.OpenFile(path, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644)
		if err != nil {
			slog.Error("Failed to open file for writing", "file", path, "error", err)
			return err
		}
		h = &FileHandle{file: f, path: path, lastShrinkTime: time.Now()}
		ds.files[id.ID()] = h
	}

	// write to file (append)
	if _, err := h.file.WriteString(string(line) + "\n"); err != nil {
		h.file.Close()
		delete(ds.files, id.ID())
		return err
	}
	h.file.Sync()
	h.lastAppendCount++

	// close file if closing is true
	if closing {
		h.file.Close()
		delete(ds.files, id.ID())
		return nil
	}

	if time.Since(h.lastShrinkTime) < ds.shrinkThresholdDuration {
		return nil
	}
	h.lastAppendCount = 0
	h.lastShrinkTime = time.Now()

	// close current file before shrinking
	h.file.Close()
	delete(ds.files, id.ID())

	// shrink file if lines exceed maxCount
	offset := 0
	b, err := os.ReadFile(h.path)
	if err != nil {
		return err
	}
	lines := strings.Split(strings.TrimRight(string(b), "\n"), "\n")

	// find the offset to keep only the last maxCount lines
	// that are within the time range of maxCount * period
	// by checking the timestamp of each line
	timeThreshold := id.OldestTime()
	for i, line := range lines {
		prd := Product{}
		if err := parseProduct(&prd, line, false); err != nil {
			slog.Warn("Failed to parse product during shrink", "line", line, "error", err)
			continue
		}
		if !prd.Time.Before(timeThreshold) {
			offset = i
			break
		}
	}
	orgLines := len(lines)
	lines = lines[offset:]
	// rewrite file with trimmed lines
	if err := os.WriteFile(h.path, []byte(strings.Join(lines, "\n")+"\n"), 0644); err != nil {
		slog.Error("Failed to rewrite shrunk file", "file", h.path, "error", err)
		return err
	}
	slog.Debug("Shrunk file", "file", h.path, "lines", fmt.Sprintf("%d -> %d", orgLines, len(lines)))
	return nil
}

func (ds *FileStorage) Load(id SeriesID, name string) ([]Product, error) {
	path := filepath.Join(ds.dir, fmt.Sprintf("%s.ts.bak", id.ID()))
	b, err := os.ReadFile(path)
	if err != nil {
		if os.IsNotExist(err) {
			return nil, nil
		}
		return nil, err
	}
	lines := strings.Split(strings.TrimRight(string(b), "\n"), "\n")
	timeThreshold := id.OldestTime()

	products := make([]Product, 0, id.MaxCount())
	for _, line := range lines {
		pd := Product{}
		if err := parseProduct(&pd, line, true); err != nil {
			slog.Warn("Failed to parse product", "line", line, "error", err)
			continue
		}
		if pd.Name != name {
			continue
		}
		if pd.Time.Before(timeThreshold) {
			continue
		}
		products = append(products, pd)
	}
	return products, nil
}

func parseProduct(pd *Product, line string, includeValue bool) error {
	obj := struct {
		Name        string         `json:"name"`
		Time        time.Time      `json:"ts"`
		Value       map[string]any `json:"value"`
		SeriesID    string         `json:"series_id"`
		SeriesTitle string         `json:"series_title"`
		Period      time.Duration  `json:"period"`
		Type        string         `json:"type"`
		Unit        Unit           `json:"unit"`
	}{}
	if err := json.Unmarshal([]byte(line), &obj); err != nil {
		slog.Warn("Failed to unmarshal product from line", "line", line, "error", err)
		return err
	}
	*pd = Product{
		Name:        obj.Name,
		Time:        obj.Time,
		Value:       nil,
		SeriesID:    obj.SeriesID,
		SeriesTitle: obj.SeriesTitle,
		Period:      obj.Period,
		Type:        obj.Type,
		Unit:        obj.Unit,
	}
	if !includeValue {
		return nil
	}

	b, err := json.Marshal(obj.Value)
	if err != nil {
		return err
	}
	switch obj.Type {
	case "counter":
		var v CounterValue
		if err := json.Unmarshal(b, &v); err != nil {
			return err
		}
		pd.Value = &v
	case "gauge":
		var v GaugeValue
		if err := json.Unmarshal(b, &v); err != nil {
			return err
		}
		pd.Value = &v
	case "timer":
		var v TimerValue
		if err := json.Unmarshal(b, &v); err != nil {
			return err
		}
		pd.Value = &v
	case "meter":
		var v MeterValue
		if err := json.Unmarshal(b, &v); err != nil {
			return err
		}
		pd.Value = &v
	case "odometer":
		var v OdometerValue
		if err := json.Unmarshal(b, &v); err != nil {
			return err
		}
		pd.Value = &v
	case "histogram":
		var v HistogramValue
		if err := json.Unmarshal(b, &v); err != nil {
			return err
		}
		pd.Value = &v
	default:
		return fmt.Errorf("unknown product type %q", obj.Type)
	}
	return nil
}
